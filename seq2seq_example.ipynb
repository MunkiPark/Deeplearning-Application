{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOAc8ZbkzLVjsjUry2ZPket",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MunkiPark/Deeplearning-Application/blob/main/seq2seq_example.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Tatoeba Project의 프랑스어-영어 이중 언어 데이터셋을 이용하여 영어를 프랑스어로 번역하는 seq2seq 모델"
      ],
      "metadata": {
        "id": "_KaHgOemoEoq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "입력 : 영어 문장<br>\n",
        "출력 : 프랑스어 문장"
      ],
      "metadata": {
        "id": "zguHZwRAquWf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##seq2seq 개요\n",
        "- 기존의 RNN은 입력 시퀀스와 똑같은 길이의 시퀀스를 출력(입력token마다 출력 token을 출력)\n",
        "- 언어 번역 시 입력과 출력의 길이가 달라지는 경우에는 사용하기 어렵다는 단점이 존재\n",
        "- seq2seq는 입력 시퀀스를 통째로 입력하고 출력 시퀀스를 통째로 받아 해당 문제점을 해결\n",
        "\n",
        "##seq2seq 구성\n",
        "- 인코더와 디코더로 구성\n",
        "- 인코더와 디코더 모두 RNN 계열(LSTM, GRU등)으로 구성\n",
        "- 인코더 : 입력 시퀀스를 입력받아 context vector를 생성\n",
        " - context vector : 입력 시퀀스의 의미를 담은 인코더의 마지막 hidden state vector\n",
        "-디코더 : context vector를 바탕으로 출력 시퀀스를 출력\n",
        " - h0로 context vector를 받고, t1에서의 input으로 start를 받아 EOS가 나올 때까지 y(t-1)을 입력으로 받으면서 시퀀스 출력"
      ],
      "metadata": {
        "id": "4BsNmOamq-OW"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "HzzHpQk_n-_N"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "import numpy as np\n",
        "import re\n",
        "import shutil\n",
        "import tensorflow as tf\n",
        "import os\n",
        "import unicodedata\n",
        "from nltk.translate.bleu_score import sentence_bleu\n",
        "from nltk.translate.bleu_score import SmoothingFunction\n",
        "num_sent_pairs = 30000 # 문장 개수"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive') #drive 연결"
      ],
      "metadata": {
        "id": "6d6zy1bMdcrE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3cb44628-b473-4f00-fd0b-987af6c7779f"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_up_logs(data_dir):\n",
        "    checkpoint_dir = os.path.join(data_dir, \"checkpoints\")\n",
        "    if os.path.exists(checkpoint_dir):\n",
        "        shutil.rmtree(checkpoint_dir, ignore_errors=True)\n",
        "        os.makedirs(checkpoint_dir)\n",
        "    return checkpoint_dir\n",
        "\n",
        "data_dir = \"./data\"\n",
        "checkpoint_dir = clean_up_logs(data_dir)"
      ],
      "metadata": {
        "id": "8jGUc0VFJU3L"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###전처리\n",
        "- 데이터셋 : dataset 폴더 안의 fra.txt\n",
        "- 인코더의 입력 : 영어 단어의 시퀀스\n",
        "- 디코더의 입력 : 프랑스어 단어 집합\n",
        "- 디코더의 출력 : 1칸씩 밀린 프랑스어 단어 시퀀스(EOS 포함)\n",
        "- 입력 전처리\n",
        " - 문자는 모두 아스키화\n",
        " - 특정 문장 부호 분리\n",
        " - 알파벳, 특정 문장 보호 이외의 모든 문자 제거\n",
        " - 문장은 모두 소문자로 변화"
      ],
      "metadata": {
        "id": "pkjP40hNeq17"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_sentence(sent):\n",
        "  sent = \"\".join([c for c in unicodedata.normalize('NFD', sent) if unicodedata.category(c) != 'Mn']) # 영어 알파벳에 없는 문자들을 정규화(조합형 문자 분리 및 결합형 문자 제거)\n",
        "  sent = re.sub(r\"([!.?])\", r\" \\1\", sent) # 문장에서 [!.?]가 나오면 뒤에 공백 추가\n",
        "  sent = re.sub(r\"[^a-zA-Z!.?]+\", r\" \", sent) # !.?을 제외한 특수문자, 숫자들을 공백으로 변환(=삭제) -> 모델이 영어 알파벳과 !.?만 다루도록 전처리\n",
        "  sent = re.sub(r\"\\s+\", \" \", sent) # 2칸 이상의 공백을 모두 1칸의 공백으로 변경\"\n",
        "  return sent.lower() #모든 대문자를 소문자로 변경하여 return"
      ],
      "metadata": {
        "id": "zAeXN2XtenoE"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- txt파일 불러오기 및 문자 시퀀스로 변경\n"
      ],
      "metadata": {
        "id": "RP5xwLKkm-s-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def download_and_read():\n",
        "  en_sents, fr_sents_in, fr_sents_out = [], [], [] # en 입력 시퀀스, fr 입력 시퀀스, fr 출력 시퀀스 리스트 선언\n",
        "  local_file = os.path.join('/',\"content\",\"drive\",\"MyDrive\",\"intern ai\",\"dataset\", \"fra.txt\") # local_file에 저장될 fra.txt의 경로를 저장\n",
        "  with open(local_file, \"r\") as fin: #local_file를 open\n",
        "    for i, line in enumerate(fin): # i : index, line : 영어-프랑스어 라인\n",
        "      en_sent, fr_sent, _ = line.strip().split('\\t') # 영어와 프랑스어 문장에서 앞 뒤의 공백들 제거한 후 tab을 기준으로 분리해서 각각 en_sent, fr_sent에 저장(문장 전체를 하나의 element로 취급하여 저장)\n",
        "      en_sent = [w for w in preprocess_sentence(en_sent).split()] # 영어 문장을 위의 전처리 함수로 처리한 후 단어들로 분리하여 en_sent에 저장(단어 1개가 list의 element1개로 취급)\n",
        "      fr_sent = preprocess_sentence(fr_sent) # 프랑스어 문장을 전처리 함수로 처리(여전히 문장 1개가 list의 element 1개)\n",
        "      fr_sent_in = [w for w in (\"BOS \"+ fr_sent).split()] # 입력 문장은 앞에 BOS를 추가하여 단어별로 분리 -> BOS를 포함한 단어들로 list 구성\n",
        "      fr_sent_out = [w for w in (fr_sent + \" EOS\").split()] # 출력 문장은 뒤에 EOS를 추가하여 단어별로 분리 _> EOS를 포함한 단어들로 list 구성\n",
        "      en_sents.append(en_sent) # en_sents에 en_sent리스트를 element로 추가(2차원 list)\n",
        "      fr_sents_in.append(fr_sent_in) #fr_snets_in에 fr_sent_in 리스트를 element로 추가\n",
        "      fr_sents_out.append(fr_sent_out) #fr_snets_out에 fr_sent_out 리스트를 element로 추가\n",
        "      if i >= num_sent_pairs -1: # index가 문장 개수를 넘어가면 for문 탈출\n",
        "        break\n",
        "  return en_sents, fr_sents_in, fr_sents_out# 생성한 2차원 리스트들을 리턴\n",
        "sents_en, sents_fr_in, sents_fr_out = download_and_read() #위의 함수를 실행하여 실제 시퀀스 sents_en, setns_fr_in, sents_fr_out 생성"
      ],
      "metadata": {
        "id": "IQVV0m60nVhc"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 입력 토큰화 및 어휘 생성\n",
        " - 문장의 각 단어들에 숫자를 할당 -> 1차원 embading"
      ],
      "metadata": {
        "id": "ljzIIyXLXF8w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer_en = tf.keras.preprocessing.text.Tokenizer(filters=\"\", lower=False) # 영어 tokenizer 객체 생성\n",
        "tokenizer_en.fit_on_texts(sents_en) # 만든 tokenizer 객체를 sents_en에 적용\n",
        "data_en = tokenizer_en.texts_to_sequences(sents_en) # 문장의 임베딩 리스트를 data_en에 저장\n",
        "data_en = tf.keras.preprocessing.sequence.pad_sequences(data_en, padding=\"post\") # 모든 시퀀스들의 길이를 맞추기 위한 패딩 추가\n",
        "\n",
        "tokenizer_fr = tf.keras.preprocessing.text.Tokenizer(filters=\"\", lower=False)# 프랑스어 tokenizer 객체 생성\n",
        "# 만든 tokenizer 객체를 입력, 출력 시퀀스에 각각 적용\n",
        "tokenizer_fr.fit_on_texts(sents_fr_in)\n",
        "tokenizer_fr.fit_on_texts(sents_fr_out)\n",
        "# 문장에 임베딩을 적용하여 저장\n",
        "data_fr_in = tokenizer_fr.texts_to_sequences(sents_fr_in)\n",
        "data_fr_out = tokenizer_fr.texts_to_sequences(sents_fr_out)\n",
        "# 모든 시퀀스들의 길이를 맞추기 위한 패딩 추가(패딩이 시퀀스 앞에 붙음)\n",
        "data_fr_in = tf.keras.preprocessing.sequence.pad_sequences(data_fr_in, padding=\"post\")\n",
        "data_fr_out = tf.keras.preprocessing.sequence.pad_sequences(data_fr_out, padding=\"post\")\n",
        "\n",
        "# 인덱스 길이=단어 종류수 저장\n",
        "vocab_size_en = len(tokenizer_en.word_index)\n",
        "vocab_size_fr = len(tokenizer_fr.word_index)\n",
        "\n",
        "# word와 index간의 변환을 위한 딕셔너리 생성\n",
        "word2idx_en = tokenizer_en.word_index\n",
        "idx2word_en = {v:k for k, v in word2idx_en.items()}\n",
        "word2idx_fr = tokenizer_fr.word_index\n",
        "idx2word_fr = {v:k for k, v in word2idx_fr.items()}\n",
        "print(\"vocab size (en): {:d}, vocab size (fr): {:d}\".format(vocab_size_en, vocab_size_fr))\n",
        "\n",
        "# 영어, 프랑스어 각 문장들의 최대 길이\n",
        "maxlen_en = data_en.shape[1]\n",
        "maxlen_fr = data_fr_out.shape[1]\n",
        "print(\"seqlen (en): {:d}, (fr): {:d}\".format(maxlen_en, maxlen_fr))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-PlQhRQ8XJ-X",
        "outputId": "eac26660-a645-40fb-e2e6-e5bfd7c768c5"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "vocab size (en): 4285, vocab size (fr): 7474\n",
            "seqlen (en): 7, (fr): 16\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 데이터를 tensor로 변환한 후 training set과 test set으로 분류"
      ],
      "metadata": {
        "id": "XiifKwjinvnI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 64 #minibatch 크기\n",
        "dataset = tf.data.Dataset.from_tensor_slices((data_en, data_fr_in, data_fr_out)) #data들을 slice한 후 tensor로 변환\n",
        "# 하나의 element가 (en, fr_in, fr_out)으로 구성\n",
        "# en, fr_in, fr_out은 모두 영단어 임베딩(숫자 1개로 구성된 벡터)으로 구성된 시퀀스(문장)\n",
        "\n",
        "dataset = dataset.shuffle(10000) # dataset을 무작위로 shuffle\n",
        "test_size = num_sent_pairs // 4 # test set의 크기 지정 : 전체 데이터의 1/4\n",
        "test_dataset = dataset.take(test_size).batch(batch_size, drop_remainder=True) # dataset에서 test_size만큼의 tensor들을 가져와 batch_size만큼의 minibatch로 분할\n",
        "train_dataset = dataset.skip(test_size).batch(batch_size, drop_remainder=True) #  dataset에서 test_size만큼 skip하여(test_size 이후의) tensor들을 가져와 batch_size만큼의 minibatch로 분할"
      ],
      "metadata": {
        "id": "3-EsYPrsp76R"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 인코더, 디코더 클래스 작성"
      ],
      "metadata": {
        "id": "34m-76n5qsbp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# vocab_size : 시퀀스의 단어 종류 수\n",
        "# num_timesteps : 시퀀스(문장)의 길이\n",
        "# embedding_dim : 임베딩 레이어의 차원(단어 1개에 배정되는 값의 개수)\n",
        "# encoder_dim : GRU 레이어의 state 차원\n",
        "class Encoder(tf.keras.Model):\n",
        "  def __init__(self, vocab_size, embedding_dim, num_timesteps, encoder_dim, **kwargs):\n",
        "    super(Encoder, self).__init__(**kwargs)\n",
        "    self.encoder_dim = encoder_dim\n",
        "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim, input_length=num_timesteps) # 임베딩 레이어 : 주어진 값들의 시퀀스를 embedding_dim 차원의 벡터 시퀀스로 변환\n",
        "    self.rnn = tf.keras.layers.GRU(encoder_dim, return_sequences=False, return_state=True) # GRU 레이어 : encoder_dim 개수의 노드를 가진 레이어\n",
        "  def call(self, x, state): # 실제 RNN 계산\n",
        "    x = self.embedding(x)\n",
        "    x, state = self.rnn(x, initial_state=state)\n",
        "    return x, state\n",
        "  def init_state(self, batch_size): # 모든 param을 0으로 초기화\n",
        "    return tf.zeros((batch_size, self.encoder_dim)) # batch_size * encoder_dim 크기의 0으로 초기화된 tensor 리턴\n",
        "\n",
        "class Decoder(tf.keras.Model):\n",
        "  def __init__(self, vocab_size, embedding_dim, num_timesteps, decoder_dim, **kwargs):\n",
        "    super(Decoder, self).__init__(**kwargs)\n",
        "    self.decoder_dim = decoder_dim\n",
        "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim, input_length=num_timesteps)\n",
        "    self.rnn = tf.keras.layers.GRU(decoder_dim, return_sequences=True, return_state=True)\n",
        "    self.dense = tf.keras.layers.Dense(vocab_size) # RNN 노드에서의 예측 단어 확률을 출력\n",
        "  def call(self, x, state):\n",
        "    x = self.embedding(x)\n",
        "    x, state = self.rnn(x, state)\n",
        "    x = self.dense(x) # 입력 x : RNN 노드의 출력(벡터) / 출력 x : RNN 노드의 출력 벡터를 일반 레이어에 넣어서 뽑아낸 각 단어들의 예측 확률\n",
        "    return x, state\n",
        "\n",
        "embedding_dim = 256\n",
        "encoder_dim, decoder_dim = 1024, 1024\n",
        "encoder = Encoder(vocab_size_en+1, embedding_dim, maxlen_en, encoder_dim) # 인코더 객체 생성\n",
        "decoder = Decoder(vocab_size_fr+1, embedding_dim, maxlen_fr, decoder_dim) # 디코더 객체 생성"
      ],
      "metadata": {
        "id": "f062p0kRqwMO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "39a95faa-10e7-497d-e9c2-90c94368d818"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 인코더, 디코더의 입력, 출력의 형태"
      ],
      "metadata": {
        "id": "5RRSFWbD8YyV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for encoder_in, decoder_in, decoder_out in train_dataset:\n",
        "  encoder_state = encoder.init_state(batch_size) # 인코더의 초기 state 생성\n",
        "  encoder_out, encoder_state = encoder(encoder_in, encoder_state)\n",
        "  decoder_state = encoder_state\n",
        "  decoder_pred, decoder_state = decoder(decoder_in, decoder_state)\n",
        "  break\n",
        "print(\"encoder input :\", encoder_in.shape)\n",
        "print(\"encoder output :\", encoder_out.shape,\"state  :\",encoder_state.shape)\n",
        "print(\"decoder output(logits) :\", decoder_pred.shape, \"state :\", decoder_state.shape)\n",
        "print(\"decoder output(labels) :\", decoder_out.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SgbXRYuC3IQr",
        "outputId": "5c991b1d-d195-44e5-f907-08779a0e613e"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "encoder input : (64, 7)\n",
            "encoder output : (64, 1024) state  : (64, 1024)\n",
            "decoder output(logits) : (64, 16, 7475) state : (64, 1024)\n",
            "decoder output(labels) : (64, 16)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- cost func 정의"
      ],
      "metadata": {
        "id": "tK4_Yi2P8dUB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def loss_fn(ytrue, ypred): # ytrue : 실제값 / ypred : 예측값\n",
        "  scce = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True) # 다중클래스 분류에서 사용되는 cost func 객체 생성/ from_logits=True : ypred가 softmax를 거치지 않은 logit이기 때문에 True로 설정\n",
        "  mask = tf.math.logical_not(tf.math.equal(ytrue, 0)) # 패딩값을 무시하는 마스크를 생성\n",
        "  mask = tf.cast(mask, dtype=tf.int64) # bool 타입인 마스크를 tf.int64 타입으로 변환 -> 손실 계산에서 가중치로 계산되어 마스크가 적용\n",
        "  loss = scce(ytrue, ypred, sample_weight=mask) # cost func에 마스크를 적용하여 계산\n",
        "  return loss"
      ],
      "metadata": {
        "id": "oiSUHgzA8fws"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- training 함수 정의"
      ],
      "metadata": {
        "id": "o9mQxynEALPI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@tf.function #tf의 그래프 실행을 활성화하는 decorator\n",
        "def train_step(encoder_in, decoder_in, decoder_out, encoder_state):\n",
        "  with tf.GradientTape() as tape: # 아래 block들에서의 미분을 자동으로 계산 및 저장\n",
        "    decoder_state = encoder_state # decoder에 context vector 전달\n",
        "    decoder_pred, decoder_state = decoder(decoder_in, decoder_state) # 디코더 실행\n",
        "    loss = loss_fn(decoder_out, decoder_pred) # cost func 실행하여 loss 계산\n",
        "  variables = (encoder.trainable_variables + decoder.trainable_variables) # encoder, decoder에서 훈련 가능한 param들을 리스트로 저장\n",
        "  gradients = tape.gradient(loss, variables) # param들과 loss들로 gradient를 계산\n",
        "  optimizer.apply_gradients(zip(gradients, variables)) # gradient를 이용하여 param들을 update\n",
        "  return loss"
      ],
      "metadata": {
        "id": "WicWL-C6AOiy"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 실제 예측을 실행하는 함수 정의\n",
        " - dataset에서 무작위로 영어 문장을 샘플링\n",
        " - 참조를 위하여 대응되는 프랑스어 문장의 label도 표시"
      ],
      "metadata": {
        "id": "7xYvQ7YfDQEE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def predict(encoder, decoder, batch_size, sents_en, data_en, sents_fr_out, word2idx_fr, idx2word_fr):\n",
        "  random_id = np.random.choice(len(sents_en)) # 무작위 영어 문장의 인덱스로 사용할 숫자 1개를 sampling\n",
        "  print(\"input : \", \" \".join(sents_en[random_id])) # 숫자를 index num으로 사용하는 영어 문장 출력\n",
        "  print(\"label : \", \" \".join(sents_fr_out[random_id])) # 해당 영어 문장에 대응되는 정답 프랑스어 문장 출력\n",
        "  encoder_in = tf.expand_dims(data_en[random_id], axis=0) # 해당하는 data_en의 시퀀스를 batch dim을 추가하여 2D tensor로 변환 -> 형태 : [1, sequence_length]\n",
        "  decoder_out = tf.expand_dims(sents_fr_out[random_id], axis=0) # 해당하는 정답 프랑스어 문장의 시퀀스를 위와 똑같이 처리 = 2D tensor로 변환\n",
        "  encoder_state = encoder.init_state(1) # 인코더의 초기 상태 설정(문장 1개만 넣을 것이므로 batch_size=1)\n",
        "  encoder_out, encoder_state = encoder(encoder_in, encoder_state) # 인코더 실행\n",
        "  decoder_state = encoder_state # context vector 전달\n",
        "  decoder_in = tf.expand_dims(tf.constant([word2idx_fr[\"BOS\"]]), axis=0) # 디코더에 입력할 BOS를 tensor로 변환\n",
        "  pred_sent_fr = [] # 디코더의 출력(=예측 프랑스어 시퀀스)를 저장할 리스트\n",
        "  while True:\n",
        "    decoder_pred, decoder_state = decoder(decoder_in, decoder_state) # 디코더 실행\n",
        "    decoder_pred = tf.argmax(decoder_pred, axis=-1) # 위에서 얻은 출력을 argmax에 넣어 가장 확률이 높은 단어 하나 선택(단, index가 들어감)\n",
        "    pred_word = idx2word_fr[decoder_pred.numpy()[0][0]] # 선택한 index에 해당하는 단어 저장\n",
        "    pred_sent_fr.append(pred_word) # 예측한 단어를 리스트에 추가\n",
        "    if pred_word == \"EOS\": # 디코더 종료 조건 : 결과값이 EOS\n",
        "      break\n",
        "    decoder_in = decoder_pred # t-1에서의 결과를 t에서의 입력으로 저장\n",
        "  print(\"predicted : \", \" \".join(pred_sent_fr)) # 예측 문장 출력"
      ],
      "metadata": {
        "id": "FDKfWanyrtaI"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 예측된 문장을 평가하는 함수 정의\n",
        " - 정답과 예측값을 BLEU 점수로 계산\n",
        " - 정답과 예측값에서 최대 4-gram에서의 유사도를 평가"
      ],
      "metadata": {
        "id": "vqIdi5wh49fR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_bleu_score(encoder, decoder, test_dataset, word2idx_fr, idx2word_fr):\n",
        "  bleu_scores = [] # 점수들을 저장할 리스트\n",
        "  smooth_fn = SmoothingFunction() # bleu 점수 계산에 사용되는 smoothing func 선택\n",
        "  for encoder_in, decoder_in, decoder_out in test_dataset: # test_dataset(시퀀스 30000개)만큼 반복\n",
        "    encoder_state = encoder.init_state(batch_size) # 인코더 param 초기화(batch_size만큼)\n",
        "    encoder_out, encoder_state = encoder(encoder_in, encoder_state) # 인코더 실행\n",
        "    decoder_state = encoder_state # context vector 전달\n",
        "    decoder_pred, decoder_state = decoder(decoder_in, decoder_state) # 디코더 실행 -> 각 단어들의 확률 분포가 pred에 저장\n",
        "    decoder_out = decoder_out.numpy() # tensor로 되어있는 확률 분포를 numpy array로 변환\n",
        "    decoder_pred = tf.argmax(decoder_pred, axis=-1).numpy() # 가장 확률이 높은 단어를 선택하여 numpy array로 변환\n",
        "    for i in range(decoder_out.shape[0]): # batch_size(시퀀스 64개)만큼 반복\n",
        "      ref_sent = [idx2word_fr[j] for j in decoder_out[i, :].tolist() if j > 0] # 정답 단어 인덱스값을 실제 단어로 변환하여 실제 문장을 리스트 형태로 저장(패딩 제외)\n",
        "      hyp_sent = [idx2word_fr[j] for j in decoder_pred[i, :].tolist() if j > 0] # 예측값 단어 인덱스값을 실제 단어로 변환하여 실제 문장을 리스트 형태로 저장(패딩 제외)\n",
        "      ref_sent = ref_sent[0:-1] # 마지막의 EOS 제거\n",
        "      bleu_score = sentence_bleu([ref_sent], hyp_sent, smoothing_function=smooth_fn.method1) # blue 점수 계산\n",
        "      bleu_scores.append(bleu_score) # minibatch 안의 시퀀스들의 점수를 리스트에 저장\n",
        "  return np.mean(np.array(bleu_scores)) # 전체 test_dataset의 모든 시퀀스들의 점수들의 평균을 계산"
      ],
      "metadata": {
        "id": "72VLXcQ97gPI"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 훈련 및 예측, 평가 실행(main 함수)"
      ],
      "metadata": {
        "id": "CJZvHF7KGGg8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = tf.keras.optimizers.Adam()\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
        "checkpoint = tf.train.Checkpoint(optimizer=optimizer, encoder=encoder, decoder=decoder)\n",
        "num_epoch = 250\n",
        "eval_scores= []\n",
        "\n",
        "for e in range(num_epoch):\n",
        "    encoder_state = encoder.init_state(batch_size)\n",
        "\n",
        "    for batch, data in enumerate(train_dataset):\n",
        "        encoder_in, decoder_in, decoder_out = data\n",
        "        # print(encoder_in.shape, decoder_in.shape, decoder_out.shape)\n",
        "        loss = train_step(\n",
        "            encoder_in, decoder_in, decoder_out, encoder_state)\n",
        "\n",
        "    print(\"Epoch: {}, Loss: {:.4f}\".format(e + 1, loss.numpy()))\n",
        "\n",
        "    if e % 10 == 0:\n",
        "        checkpoint.save(file_prefix=checkpoint_prefix)\n",
        "\n",
        "    predict(encoder, decoder, batch_size, sents_en, data_en,\n",
        "        sents_fr_out, word2idx_fr, idx2word_fr)\n",
        "\n",
        "    eval_score = evaluate_bleu_score(encoder, decoder, test_dataset, word2idx_fr, idx2word_fr)\n",
        "    print(\"Eval Score (BLEU): {:.3e}\".format(eval_score))\n",
        "    # eval_scores.append(eval_score)\n",
        "\n",
        "checkpoint.save(file_prefix=checkpoint_prefix)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 623
        },
        "id": "fej9S1uTF7hT",
        "outputId": "8bbb04e3-9d79-4176-9f4b-5c1a6b26c15f"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/optimizers/base_optimizer.py:774: UserWarning: Gradients do not exist for variables ['encoder_2/embedding_4/embeddings', 'encoder_2/gru_4/gru_cell/kernel', 'encoder_2/gru_4/gru_cell/recurrent_kernel', 'encoder_2/gru_4/gru_cell/bias'] when minimizing the loss. If using `model.compile()`, did you forget to provide a `loss` argument?\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1, Loss: 1.3132\n",
            "input :  i like sauerkraut .\n",
            "label :  j aime la choucroute . EOS\n",
            "predicted :  je suis un peu . EOS\n",
            "Eval Score (BLEU): 1.530e-02\n",
            "Epoch: 2, Loss: 1.1809\n",
            "input :  i m working .\n",
            "label :  je suis en train de travailler . EOS\n",
            "predicted :  je ne suis pas si . EOS\n",
            "Eval Score (BLEU): 1.773e-02\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-35-346b90953cb8>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0mencoder_in\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder_in\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder_out\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0;31m# print(encoder_in.shape, decoder_in.shape, decoder_out.shape)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m         loss = train_step(\n\u001b[0m\u001b[1;32m     14\u001b[0m             encoder_in, decoder_in, decoder_out, encoder_state)\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    831\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    832\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 833\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    834\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    835\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    867\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    868\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 869\u001b[0;31m       return tracing_compilation.call_function(\n\u001b[0m\u001b[1;32m    870\u001b[0m           \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_no_variable_creation_config\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    871\u001b[0m       )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py\u001b[0m in \u001b[0;36mcall_function\u001b[0;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[1;32m    137\u001b[0m   \u001b[0mbound_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m   \u001b[0mflat_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munpack_inputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbound_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 139\u001b[0;31m   return function._call_flat(  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m    140\u001b[0m       \u001b[0mflat_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m   )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/polymorphic_function/concrete_function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, tensor_inputs, captured_inputs)\u001b[0m\n\u001b[1;32m   1320\u001b[0m         and executing_eagerly):\n\u001b[1;32m   1321\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1322\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_inference_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_preflattened\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1323\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1324\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py\u001b[0m in \u001b[0;36mcall_preflattened\u001b[0;34m(self, args)\u001b[0m\n\u001b[1;32m    214\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mcall_preflattened\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mSequence\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m     \u001b[0;34m\"\"\"Calls with flattened tensor inputs and returns the structured output.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 216\u001b[0;31m     \u001b[0mflat_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    217\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpack_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflat_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py\u001b[0m in \u001b[0;36mcall_flat\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    249\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mrecord\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_recording\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m           \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_bound_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 251\u001b[0;31m             outputs = self._bound_context.call_function(\n\u001b[0m\u001b[1;32m    252\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m                 \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/context.py\u001b[0m in \u001b[0;36mcall_function\u001b[0;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[1;32m   1681\u001b[0m     \u001b[0mcancellation_context\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcancellation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1682\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcancellation_context\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1683\u001b[0;31m       outputs = execute.execute(\n\u001b[0m\u001b[1;32m   1684\u001b[0m           \u001b[0mname\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"utf-8\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1685\u001b[0m           \u001b[0mnum_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     51\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     54\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     55\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    }
  ]
}